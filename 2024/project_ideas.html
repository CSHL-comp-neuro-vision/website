<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Project Ideas &mdash; CSHL: Computational Neuroscience Vision  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Course T-Shirt" href="course_tshirt_submissions.html" />
    <link rel="prev" title="Lecture Notes" href="lecture_notes.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> CSHL: Computational Neuroscience Vision
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">2024 Course</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture_notes.html">Lecture Notes</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Project Ideas</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#ej-chichilnisky">EJ Chichilnisky</a></li>
<li class="toctree-l2"><a class="reference internal" href="#john-serences">John Serences</a></li>
<li class="toctree-l2"><a class="reference internal" href="#eero-simoncelli">Eero Simoncelli</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stephanie-palmer">Stephanie Palmer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#jonathan-pillow">Jonathan Pillow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#jacob-yates">Jacob Yates</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lea-duncker">Lea Duncker</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ruth-rosenholtz">Ruth Rosenholtz</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rachel-denison">Rachel Denison</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tony-movshon">Tony Movshon</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kate-bonnen">Kate Bonnen</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stefan-treue">Stefan Treue</a></li>
<li class="toctree-l2"><a class="reference internal" href="#emily-cooper">Emily Cooper</a></li>
<li class="toctree-l2"><a class="reference internal" href="#marlene-cohen">Marlene Cohen</a></li>
<li class="toctree-l2"><a class="reference internal" href="#madineh-sedigh-sarvestani">Madineh Sedigh-Sarvestani</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ione-fine">Ione Fine</a></li>
<li class="toctree-l2"><a class="reference internal" href="#emma-alexander">Emma Alexander</a></li>
<li class="toctree-l2"><a class="reference internal" href="#jennifer-groh">Jennifer Groh</a></li>
<li class="toctree-l2"><a class="reference internal" href="#taraz-lee">Taraz Lee</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mariam-aly">Mariam Aly</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kohitij-kar">Kohitij Kar</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lindsey-glickfeld">Lindsey Glickfeld</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="course_tshirt_submissions.html">Course T-Shirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="people.html">People</a></li>
<li class="toctree-l1"><a class="reference internal" href="photos.html">Course photos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Past Courses &amp; Alumni</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../past/past_courses.html">Past Courses &amp; Alumni</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CSHL: Computational Neuroscience Vision</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Project Ideas</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/2024/project_ideas.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="project-ideas">
<h1>Project Ideas<a class="headerlink" href="#project-ideas" title="Permalink to this heading"></a></h1>
<section id="ej-chichilnisky">
<h2>EJ Chichilnisky<a class="headerlink" href="#ej-chichilnisky" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Download Pillow data set. Fit GLM. Test if we can do better with some kind of subunit model fit (Shah 2020 paper)?</p></li>
</ol>
</section>
<section id="john-serences">
<h2>John Serences<a class="headerlink" href="#john-serences" title="Permalink to this heading"></a></h2>
<p>Use a continuous time recurrent neural network (example code in python tutorials) to build a network that performs a simple delayed match to sample (DMTS) task (or some other task of your own design). Does the network naturally exhibit within-trial dynamics after training? Does it settle into a crystalized state after reaching asymptotic performance, or does it continue to explore the solution space? What role do these dynamics play in successfully learning your task? And last, what parameters drive increased dynamics in the networks in the support of more efficient processing? For example, in a DMTS memory task, you could look at how the network adaptively prepares to compare the second stimulus to the first during the delay period and what factors might encourage the network to become more dynamic in support of more efficient task performance (e.g. manipulations of the loss function, changes in connectivity, E/I balance, etc)?</p>
</section>
<section id="eero-simoncelli">
<h2>Eero Simoncelli<a class="headerlink" href="#eero-simoncelli" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Metamers: given a model and a reference image, stochastically generate a new image whose model representation is identical to that of the reference image.  [ref:  Freeman &amp; Simoncelli 2011]</p></li>
<li><p>Eigendistortions: given a model and a reference image, compute the image perturbation that produces the smallest and largest changes (in terms of Euclidean distance) in the model response space.  [ref: Berardino et al 2017]</p></li>
<li><p>Maximal differentiation (MAD) competition: given two models that measure distance between images and a reference image, generate pairs of images that optimally distinguish the models. Specifically, synthesize a pair of images that the first model says are equi-distant from the reference while the second model says they are maximally/minimally distant from the reference. Synthesize a second pair with the roles of the two models reversed.  [ref: Wang &amp; Simoncelli 2008]</p></li>
<li><p>Geodesics: given a model and two images, attempt to synthesize a sequence of images that lie on the shortest (“geodesic”) path in the model’s representation space. [ref. Henaff &amp; Simoncelli 2016]</p></li>
</ol>
<p>(note from Dan: not really a project idea, but a tool you could use in a project)
My lab has produced a python library of tools to explore vision models by  synthesizing novel informative images.
This includes Metamers (as per my talk yesterday), Eigendistortions (Berardino 2017), Maximal differentiation (MAD) competition (Wang 2008), and Geodesics (Henaff 2016). Might be useful for some students in the course.  If you want to link it from Slack or from the course GitHub, it’s [here]https://github.com/LabForComputationalVision/plenoptic/: Kate Bonnen is an expert user and someone to ask questions about it.</p>
</section>
<section id="stephanie-palmer">
<h2>Stephanie Palmer<a class="headerlink" href="#stephanie-palmer" title="Permalink to this heading"></a></h2>
<p>Project ideas:
Open the provided natural movie and associated retinal data from the larval salamander. Use DeepLabCut, your favorite tracking algorithm, or your own hand-tracking to map out the trajectories of 1-5 objects in one of the five scenes. Does the retinal population have information about these trajectories? Compute the correlation between the population firing and the past, present, and future position of your chosen object. At what lag is the correlation maximal? What other metrics could you use to quantify this?</p>
<p>Try to animate a circle so that it looks “alive”. What kind of features do you want your pet circle’s trajectory to have? Try making it unpredictable; try making it oscillate. Can you make a trajectory that would pass a Turing test?</p>
<p>Find a good open-source model retina and play it a natural movie. How much can you modify the movie before you can tell the difference in the retina’s response. Find the retina’s metamers! This riffs on ideas you’ll hear from Eero and EJ.</p>
</section>
<section id="jonathan-pillow">
<h2>Jonathan Pillow<a class="headerlink" href="#jonathan-pillow" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Temporal vs. rate coding in retina (or any other dataset you’d like to examine).
How much information is carried in the precise timing of spikes vs. in slowly-fluctuating spike rates?  One way to address this question is to decode spikes under models that incorporate vs. ignore precise spike timing information.
Project idea;  compare decoding under an LNP model and a Poisson GLM with spike history filters. How much (if any) additional information can you recover about the stimulus when you incorporate spike history?</p></li>
</ol>
<p>Relevant refs:</p>
<ul class="simple">
<li><p>Pillow et al. “Spatio-temporal correlations and visual signalling in a complete neuronal population”.  Nature (2008).</p></li>
</ul>
<p>See also: <a class="reference external" href="https://github.com/pillowlab/GLMspiketraintutorial">GLM tutorial code</a>.</p>
<ol class="arabic simple" start="2">
<li><p>Can a Poisson GLM exhibit divisive normalization?
Divisive normalization is one of the putative “canonical computations” carried out in the brain, but we still lack a good computational understanding of how it’s carried out, or how to infer statistical models that can exhibit divisive normalization.</p></li>
</ol>
<p>The Poisson generalized linear model (GLM) for spike trains provides a simple, tractable statistical model of spike trains. But can it exhibit divisive normalization?</p>
<p>Relevant refs:</p>
<p>• <a href="../_static/pdfs/pillow/carandini-heeger-2011-natrevneuro.pdf">Carandini &amp; Heeger (2011) Normalization as a canonical neural computation. <em>Nat Rev Neurosci, 13</em>:51-62. DOI:10.1038/nrn3136.</a>
<a href="../_static/pdfs/pillow/carandini-heeger-1994-science.pdf">Carandini &amp; Heeger. (1994). Summation and division by neurons in primate visual cortex. <em>Science, 264</em>(5183):1333-1336. DOI:10.1126/science.8191289.</a>
• <a href="../_static/pdfs/pillow/Pillow_etal_2008.pdf">Pillow et al. (2008). Spatio-temporal correlations and visual signalling in a complete neuronal population. <em>Nature, 454</em>(21). DOI:10.1038/nature07140.</a></p>
<ol class="arabic simple" start="3">
<li><p>Compare GLM and deep neural networks - try out deep learning on some real neural data. See e.g.:
• <a class="reference external" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006897">Deep convolutional models improve predictions of macaque V1 responses to natural images</a>.</p></li>
</ol>
<p>• <a class="reference external" href="https://openreview.net/forum?id=HkEI22jeg">Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses</a>.</p>
<p>• A recent paper argued that “modern machine learning” outperforms GLMs in many cases (although in the detailed results shown, GLM outperforms a deep neural network on most of the examples considered). Download their datasets and see if you can do better:
• <a class="reference external" href="https://www.frontiersin.org/articles/10.3389/fncom.2018.00056/full">Paper</a>
• <a class="reference external" href="https://github.com/KordingLab/spykesML/tree/master/data">Data</a>
(There are a lot of possible ways to think about improving: the paper did not consider different nonlinearities and made only limited attempts to select GLM features).</p>
</section>
<section id="jacob-yates">
<h2>Jacob Yates<a class="headerlink" href="#jacob-yates" title="Permalink to this heading"></a></h2>
</section>
<section id="lea-duncker">
<h2>Lea Duncker<a class="headerlink" href="#lea-duncker" title="Permalink to this heading"></a></h2>
</section>
<section id="ruth-rosenholtz">
<h2>Ruth Rosenholtz<a class="headerlink" href="#ruth-rosenholtz" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Combining predictions of peripheral vision with fixation data. We provide a subset of 100 images from the COCO-Periph dataset. The visualizations and code provided allow one to create visualizations of the information available for a (mostly) arbitrary fixation. We also provide eye tracking data for these images, from the COCO-Search 18 dataset from Greg Zelinsky. Observers executed these fixations while performing a search task. This combination of model predictions and eye tracking data should enable a number of interesting projects. For instance, consider the sequence of fixation locations, {(fx_i, fy_i), (fx_i+1, fy_i+1)}. Was the object at (fx_i+1, fy_i+1) likely identifiable when fixating (fx_i, fy_i), according to the peripheral vision model? In which case what might be the purpose of that saccade?</p></li>
<li><p>Many aspects of peripheral vision get worse in a roughly linear way. Acuity worsens as an approximately linear function of eccentricity, as does the critical spacing of crowding, as do hyperacuity and other aspects of vision. Van Essen and Anderson (1995) and others have suggested that this makes the information encoded about a stimulus relatively invariant to viewing distance. Under what circumstances does this hold, and when does it fail? Does it matter that the different linear functions have different slopes and intercepts?</p></li>
</ol>
</section>
<section id="rachel-denison">
<h2>Rachel Denison<a class="headerlink" href="#rachel-denison" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Maximally informative time points. We don’t need to pay attention at every moment, because sometimes nothing interesting is happening. Given a movie clip and a task that you specify, build a system that will learn the time points (frames in the movie) that are most informative for completing the task. You can use movies and tasks with varying degrees of complexity, so consider starting as simply as possible and building up from there!</p></li>
<li><p>Timing a brain process. Many aspects of neural control—including the control of visual attention—involve generating precisely timed brain activity. Build a neural network that delivers an output spike at a specified temporal interval following an input stimulus. Consider trying different timing mechanisms used by the brain, including ramping, oscillations, and trajectories through a state-space. How does the temporal precision of your output spike depend on the length of the interval? If you have solved the problem for one fixed interval, can you build a network that can generate an arbitrary temporal interval? Can you build a network that can learn a new temporal interval?</p></li>
<li><p>Contending with temporal uncertainty. Sometimes we know approximately when an important event will occur, but we don’t have perfect timing information. Such temporal uncertainty can arise from internal sources, like imprecision in our timing estimates, or external sources, like stochasticity in event times (e.g., a bus that comes approximately every 15 minutes but can be a little early or late). Consider a task in which a single stimulus appears with temporal uncertainty on each trial and the observer has to make some report on it. Given some source of temporal uncertainty that you specify, how should the observer allocate attention over time to maximize task performance across trials? You will need to make some assumptions about 1) how attention can be allocated across time (if there are no constraints, the observer could just attend all the time) and 2) how the amount of attention at the time of the stimulus relates to task performance.</p></li>
<li><p>On a different note, if you are interested in visual metacognition, the Confidence Database is a great resource of behavioral data with ~150 datasets and counting. There are many possible analysis projects that could be done using <a href="https://osf.io/s46pr/">these data</a> by  <a href="../_static/pdfs/denison/rahnev-et-al-2020-nhb.pdf">Rahnev et al. (2020). The Confidence Database. <em>Nature Human Behaviour, 4</em>(3):317-325.</a></p></li>
</ol>
</section>
<section id="tony-movshon">
<h2>Tony Movshon<a class="headerlink" href="#tony-movshon" title="Permalink to this heading"></a></h2>
</section>
<section id="kate-bonnen">
<h2>Kate Bonnen<a class="headerlink" href="#kate-bonnen" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Continuous matching experiment.  Employing a tracking method to continuously match a visual feature (e.g., orientation).</p></li>
<li><p>Continuous subjective measurements.  Measuring subjective signals (e.g., target detection in noise — what is your confidence that you can see the target?).</p></li>
<li><p>Using a sampling approach to fit a Kalman filter with varying parameters (i.e. what if the detectability of the stimulus changes over time).</p></li>
<li><p>Implement a LQG with subjective beliefs (as in this paper) and then try to design a set of conditions that will maximally expose subjective beliefs about the dynamics of the trajectory.</p></li>
</ol>
<p><strong>Suggestions related to Eero’s plenoptic code repository</strong></p>
<p><strong>From Billy</strong></p>
<ol class="arabic simple">
<li><p>I’ve been wanting to extend metamers to generate several metamers at once that are as maximally different (in either pixel space, model space, or by some other metric) from each other as possible</p></li>
</ol>
<p><strong>From Nikhil</strong></p>
<ol class="arabic simple">
<li><p>One thing that could be interesting is to use MAD competition to generate images that can differentiate between deepnet layers and the portilla model. People have done a lot just synthesizing from both models but could be interesting to see if there are actual fundamental differences in the representations that are captured by each.</p></li>
<li><p>In the same vain, I’d like to see how well the portilla model does at discriminative tasks like classification. Since a lot of work is saying deepnets trained for classification are texture biased it would be interesting to just see how well a texture model does at object recognition for example on imagenet. Maybe it can provide a better baseline to then evaluate performance of recognition models that capture more than just texture statistics.</p></li>
</ol>
<p><strong>From Pierre-Étienne</strong></p>
<ol class="arabic simple">
<li><p>It would be interesting to use eigendistortions to understand the computational role of layers deep inside a network (/biological system). Instead of end2end maximally minimally visible distortions, one could ask for changes in an image that result in changes of the representation in layer i that is most / least visible to layer j. Same question could be asked of unit i and unit j. Same question can be asked for the curvature of sequences (geodesics), which would help understand what piece in a net do straightening and how.</p></li>
<li><p>An idea to help understand the shape of the natural image manifold is to use triplets of related images and a model to measure the angles between the geodesics that link them, the shapes of these triangles would reveal the local curvature of the space as seen by the model. A texture aware (full reference) perceptual metric would compare isolated features accurately (allowing for shifts and changes in luminance/contrast) while comparing textures loosely (using local statistics only)- and gradually interpolate between these two extremes. A step in that direction is to build a normalized steerable pyramid distance (an evolution of the normalized laplacian pyramid) with a texture/feature gate given by the ratio of local representation std to local representation mean. One would then show that the metric is tolerant to resamplings of texture patterns and sensitive to the exact form of isolated features, just like our visual experience (ie do psychophysics).</p></li>
</ol>
<p><strong>From Xinyuan Zhao</strong></p>
<ol class="arabic simple">
<li><p>There is a perceptual distance metric called normalized steerable pyramid distance (NSPD) in the code. It is analogous to the published work of normalized Laplacian pyramid distance (NLPD), only replacing the Laplacian pyramid with the steerable pyramid. I have spent some time working on it, but did not get it to perform very well (now about the same as NLPD). Maybe it can be improved by summer school students.</p></li>
</ol>
</section>
<section id="stefan-treue">
<h2>Stefan Treue<a class="headerlink" href="#stefan-treue" title="Permalink to this heading"></a></h2>
</section>
<section id="emily-cooper">
<h2>Emily Cooper<a class="headerlink" href="#emily-cooper" title="Permalink to this heading"></a></h2>
<p>I’ve shared a (non-exhaustive) list of some public databases of natural images, depth maps, and eye movements that may be useful for projects <a href="https://docs.google.com/document/d/1bVTdvXXoGN4Ya4mutdEBQOQdmO6tc68uHHIccrvdlTI/edit?usp=sharing">here</a>:</p>
<ol class="arabic simple">
<li><p>What does your next fixation point look like?
Using the DOVES dataset of eye movements during free-viewing of calibrated natural images (see <a href="https://live.ece.utexas.edu/research/doves/">Google Doc</a>), select pairs of fixation points in temporal sequence and create small image patches centered on those fixation points. Characterize the visual similarity (or dissimilarity) of these pairs of sequentially fixated image patches using any approaches you’d like (e.g., mutual information, difference in slope of Fourier power spectrum, difference in orientation spectrum, difference in RMS contrast, SSIM). Next, compare the distribution of these measures to a distribution derived from randomly selected pairs of image patches. Are sequentially fixated image patches more similar or different from random pairs of image patches based on any of these measures? Be sure to control for the distance between pairs (image patches that are closer to each other are known to be more similar). If you do find a pattern, does it extend to fixated points with larger separations in time? You could try this analysis on one of the other eye movements datasets, but keep in mind that the pixel intensity values in Hollywood movies are not necessarily linear with respect to light in the world.</p></li>
<li><p>How far away is your next fixation?
When engaged in natural tasks, people tend to fixate points that are relatively close to them, as compared to a random sample of points from their surrounding environment. Using the UT Austin Natural Image Databases (see <a href="https://natural-scenes.cps.utexas.edu/db.shtml">Google Doc</a> – just the subset with co-registered images and depth maps, scroll down to “Stereo Image and Range Data Collection”), investigate potential low level fixation strategies that might recapitulate this behavior. First, simulate a set of random fixation points and plot the distribution of associated scene distances from the depth maps – this should generally match the overall distribution of distances found in the scenes. Next, try biasing your fixation points based on low level properties of the co-registered images run the scene images. For example, you could run the images through an edge detection algorithm and select a random sample of points that fall on luminance edges. You could calculate the local RMS contrast of points and weight your fixation sampling strategy towards higher contrast image regions. You could even assert that people only fixate points that are red. Do any of these strategies result in the near-distance bias we observe in natural fixations?</p></li>
</ol>
</section>
<section id="marlene-cohen">
<h2>Marlene Cohen<a class="headerlink" href="#marlene-cohen" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Generate a fake multineuron data set. This should include spike count, BOLD, or calcium imaging responses of n neurons (or voxels) on m trials. You could use models from any of several tutorials to generate these responses. (I know that ChoiceProbabilityTutorial will do this for a population of MT neurons.) From this fake data set, calculate:</p></li>
</ol>
<p>a. The average correlation between each pair of neurons (this could be noise or signal correlation, depending on the data set you generated).\</p>
<p>b. The dimensionality of the shared variability (e.g. by calculating the proportion variance explained by the first k principal components for k=1:n)\</p>
<p>c. For the overachievers among you, calculate some fancy measurements too, such as communication subspace dimensionality if you modeled multiple brain areas (good code <a href="https://github.com/joao-semedo/communication-subspace">here</a>) or average or modal controllability (good code at #3 <a href="https://complexsystemsupenn.com/codedata">here</a>; start with the correlation matrix with the diagonal set to 0 instead of 1). If you would like to experiment with topological data analysis, I can provide code for that too.\</p>
<p>d. Next, make some change to your model that will affect the shared variability (maybe add in a common noise source, or change some model parameters) and repeat the calculations in #a-c. Can you gain some insight into how those numbers are related?\</p>
<p>e. Possible publishable extension: Do these calculations in a real data set (yours? Or several from my lab are available if you would like). Which ways of quantifying shared variability are most closely associated with behavior, stimuli, task condition, or some other quantity you think is important?</p>
</section>
<section id="madineh-sedigh-sarvestani">
<h2>Madineh Sedigh-Sarvestani<a class="headerlink" href="#madineh-sedigh-sarvestani" title="Permalink to this heading"></a></h2>
</section>
<section id="ione-fine">
<h2>Ione Fine<a class="headerlink" href="#ione-fine" title="Permalink to this heading"></a></h2>
</section>
<section id="emma-alexander">
<h2>Emma Alexander<a class="headerlink" href="#emma-alexander" title="Permalink to this heading"></a></h2>
</section>
<section id="jennifer-groh">
<h2>Jennifer Groh<a class="headerlink" href="#jennifer-groh" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>How does a brain create/read out a multiplexed code?  Figure 7 of Caruso et al 2018 provides a back-of-the-envelope sketch of a circuit model of some possibilities for the de-multiplexing.  Try implementing one of these, or come up with your own!</p></li>
</ol>
</section>
<section id="taraz-lee">
<h2>Taraz Lee<a class="headerlink" href="#taraz-lee" title="Permalink to this heading"></a></h2>
</section>
<section id="mariam-aly">
<h2>Mariam Aly<a class="headerlink" href="#mariam-aly" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Design and program a behavioral experiment that assesses relational attention and/or perception. The task should be designed to tax hippocampal representations, based on the knowledge you acquired from the assigned readings above. You can use PsychoPy, PsychToolBox for Matlab, Gorilla, jsPsych, Pavlovia, or similar software. \</p></li>
</ol>
<p>For inspiration, read about the behavioral tasks in these studies:<br />
• <a href="../_static/pdfs/aly/cordova-turkbrowne-aly-2019-hippocampus.pdf">Córdova N.I., Turk‐Browne N.B., &amp; Aly M. (2019). Focusing on what matters: Modulation of the human hippocampus by relational attention. <em>Hippocampus,29</em>(11):1025-37. DOI: 10.1002/hipo.23082.</a><br />
• <a href="../_static/pdfs/aly/ruiz-meager-agarwal-aly-2020-jocn.pdf">Ruiz N.A., Meager M.R., Agarwal S., Aly M. The medial temporal lobe is critical for spatial relational perception. <em>Journal of Cognitive Neuroscience, 32</em>(9):1780-95. DOI:10.1162/jocn_a_01583.</a>\</p>
</section>
<section id="kohitij-kar">
<h2>Kohitij Kar<a class="headerlink" href="#kohitij-kar" title="Permalink to this heading"></a></h2>
<p>For background, I added two tutorials:</p>
<ol class="arabic simple">
<li><p>A <a class="reference external" href="https://github.com/CSHL-comp-neuro-vision/tutorials/blob/main/python/NoiseCorrection/noise_correction_demo_py.ipynb">Jupyter notebook</a> with a tutorial demonstrating the effects of misrepresenting relationships between variables due to noise in their estimates and how to retrieve the true relationship using noise correction techniques.</p></li>
<li><p><a class="reference external" href="https://github.com/kohitij-kar/prediction_demo">A tutorial on predicting neural activity using deep net features.</a></p></li>
</ol>
<p>Project idea:</p>
<ol class="arabic simple">
<li><p>Use one artificial neural network (Target) as a stand-in for human/monkey behavior or neural activity. Then optimize the experimental design, using methods like controversial stimuli generation, to best discriminate other models of that Target.</p></li>
</ol>
</section>
<section id="lindsey-glickfeld">
<h2>Lindsey Glickfeld<a class="headerlink" href="#lindsey-glickfeld" title="Permalink to this heading"></a></h2>
<p>In <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0896627321007856">Barbera et al.</a>, we used a simple subunit based, Hubel and Wiesel style model to predict V1 responses to gratings and plaids, and then to test the effect of mask phase on these responses.</p>
<ol class="arabic simple">
<li><p>This model built V1 cells that only have one (excitatory) subunit and orientation preference/selectivity is defined by the elongation of the long axis of the RF. However, V1 neurons typically have an “on” and an “off” subunit, and in the mouse orientation preference/selectivity is defined by the axis of overlap of these subunits. Build a new version of this model with on/off subunits and test whether this changes (A) the magnitude of cross orientation interactions and (B) the sensitivity of those interactions to mask phase.</p></li>
<li><p>In cat/primate V1, plaid stimuli drive responses that are component selective.  However, in mouse V1, responses can be either pattern and component selective. Use this simple subunit model to (A) determine whether V1 neurons are component and/or pattern selective and (B) test whether these responses are sensitive to mask phase.</p></li>
</ol>
<p>The code for the model in Barbera et al. can be found <a href="https://figshare.com/collections/BarberaPriebeGlickfeld_Neuron_2022/5677225/">here</a>, see <em>Figure4_model_code.m</em>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_notes.html" class="btn btn-neutral float-left" title="Lecture Notes" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="course_tshirt_submissions.html" class="btn btn-neutral float-right" title="Course T-Shirt" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Eline Kupers, Catrina Hacker, Declan Rowley.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>